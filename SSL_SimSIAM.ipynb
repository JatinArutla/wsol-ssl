{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hEn3X7AgEXel"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip '/content/drive/MyDrive/Kaggle/data.zip' -d '/content/'"
      ],
      "metadata": {
        "id": "p8UIfRaiGCl0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "import os\n",
        "from cv2 import imread, createCLAHE # read and equalize images\n",
        "from glob import glob\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "from pandas.core.common import flatten\n",
        "\n",
        "import os\n",
        "import torch\n",
        "import pandas as pd\n",
        "from skimage import io, transform\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms, utils\n",
        "\n",
        "# Ignore warnings\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "plt.ion()   # interactive mode"
      ],
      "metadata": {
        "id": "H8by_ChwGN7e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from __future__ import print_function, division\n",
        "\n",
        "# pytorch imports\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.optim import lr_scheduler\n",
        "from torch.autograd import Variable\n",
        "import torchvision\n",
        "from torchvision import datasets, models, transforms\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms, utils\n",
        "\n",
        "# image imports\n",
        "from skimage import io, transform\n",
        "from PIL import Image\n",
        "\n",
        "# general imports\n",
        "import os\n",
        "import time\n",
        "from shutil import copyfile\n",
        "from shutil import rmtree\n",
        "\n",
        "# data science imports\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import csv"
      ],
      "metadata": {
        "id": "wvJlzcs2GSUb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install lightly"
      ],
      "metadata": {
        "id": "5svrKWk6GT57"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from itertools import chain\n",
        "import random\n",
        "np.random.seed(25)\n",
        "\n",
        "df = pd.read_csv('/content/Data_Entry_2017.csv')\n",
        "df.drop(['OriginalImage[Width', 'Height]', 'OriginalImagePixelSpacing[x', 'y]', 'Unnamed: 11'], axis=1, inplace=True)\n",
        "num_obs = len(df)\n",
        "# print('Number of observations:',num_obs)\n",
        "\n",
        "my_glob = glob('/content/images*/images/*.png')\n",
        "# print('Number of Observations: ', len(my_glob))\n",
        "\n",
        "full_img_paths = {os.path.basename(x): x for x in my_glob}\n",
        "df['full_path'] = df['Image Index'].map(full_img_paths.get)\n",
        "\n",
        "train_val_list = pd.read_csv('/content/train_val_list.txt', header=None, names = ['image_list'])\n",
        "test_list = pd.read_csv('/content/test_list.txt', header=None, names = ['image_list'])\n",
        "\n",
        "train = df[df['Image Index'].isin(train_val_list['image_list'].values)].reset_index(drop=True)\n",
        "test = df[df['Image Index'].isin(test_list['image_list'].values)].reset_index(drop=True)\n",
        "\n",
        "labels_discard = ['Consolidation', 'Edema', 'Emphysema', 'Fibrosis', 'Hernia', 'Pleural_Thickening']\n",
        "for i in labels_discard:\n",
        "    train = train[~train['Finding Labels'].str.contains(i)]\n",
        "    test = test[~test['Finding Labels'].str.contains(i)]\n",
        "\n",
        "# train = pd.concat([train[~train['Finding Labels'].str.contains('No Finding')],\n",
        "#                   train[train['Finding Labels'].str.contains('No Finding')].drop_duplicates(subset=['Finding Labels', 'Patient ID', 'View Position'], keep='first')]).sort_values(by='Image Index')\n",
        "\n",
        "def one_hot_enc(df):\n",
        "    df['Finding Labels'] = df['Finding Labels'].map(lambda x: x.replace('No Finding', ''))\n",
        "    all_labels = np.unique(list(chain(*df['Finding Labels'].map(lambda x: x.split('|')).tolist())))\n",
        "    for c_label in all_labels:\n",
        "        if len(c_label)>1: # leave out empty labels\n",
        "            df[c_label] = df['Finding Labels'].map(lambda finding: 1 if c_label in finding else 0)\n",
        "    return df\n",
        "\n",
        "train = one_hot_enc(train)\n",
        "test = one_hot_enc(test)\n",
        "\n",
        "train_image_paths = list(flatten(train['full_path'].values))\n",
        "test_image_paths = list(flatten(test['full_path'].values))\n",
        "\n",
        "l = np.unique(train['Patient ID'].values)\n",
        "np.random.shuffle(l)\n",
        "cut = int(np.round(((90/100)*len(l)), decimals=0))\n",
        "train_values = l[:cut]\n",
        "val_values = l[cut:]\n",
        "train_dict = dict.fromkeys(train_values, 'train')\n",
        "train_dict.update(dict.fromkeys(val_values, 'val'))\n",
        "train['fold'] = train['Patient ID'].map(train_dict.get)\n",
        "\n",
        "train = train[['full_path', 'Image Index', 'fold', 'Atelectasis', 'Cardiomegaly', 'Effusion', 'Infiltration',\n",
        "               'Mass', 'Nodule', 'Pneumonia', 'Pneumothorax']]\n",
        "test['fold'] = 'test'\n",
        "test = test[['full_path', 'Image Index', 'fold', 'Atelectasis', 'Cardiomegaly', 'Effusion', 'Infiltration',\n",
        "             'Mass', 'Nodule', 'Pneumonia', 'Pneumothorax']]\n",
        "train.reset_index(drop=True, inplace=True)\n",
        "test.reset_index(drop=True, inplace=True)"
      ],
      "metadata": {
        "id": "D_gVfjA5GXxP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "\n",
        "from torch.utils.data import (\n",
        "    Dataset,\n",
        "    DataLoader,\n",
        ")\n",
        "\n",
        "from torchvision.datasets import ImageFolder\n",
        "from torchvision.transforms import (\n",
        "    RandomResizedCrop,\n",
        "    RandomHorizontalFlip,\n",
        "    ColorJitter,\n",
        "    RandomGrayscale,\n",
        "    RandomApply,\n",
        "    Compose,\n",
        "    GaussianBlur,\n",
        "    ToTensor,\n",
        ")\n",
        "import torchvision.models as models\n",
        "\n",
        "import os\n",
        "import glob\n",
        "import time\n",
        "from skimage import io\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "lz2F8EziGcZ_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image as img\n",
        "\n",
        "class CreateDataset(Dataset):\n",
        "\n",
        "    def __init__(self, train, fold, transform):\n",
        "        self.df = train\n",
        "        self.transform = transform\n",
        "        self.df = self.df[self.df['fold'] == fold]\n",
        "\n",
        "        self.df = self.df.set_index(\"Image Index\")\n",
        "        self.PRED_LABEL = ['Atelectasis',\n",
        "                           'Cardiomegaly',\n",
        "                           'Effusion',\n",
        "                           'Infiltration',\n",
        "                           'Mass',\n",
        "                           'Nodule',\n",
        "                           'Pneumonia',\n",
        "                           'Pneumothorax']\n",
        "        RESULT_PATH = \"results/\"\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        if torch.is_tensor(idx):\n",
        "            idx = idx.tolist()\n",
        "\n",
        "        image = img.open(self.df.iloc[idx, 0])\n",
        "        image = image.convert('RGB')\n",
        "\n",
        "        label = np.zeros(len(self.PRED_LABEL), dtype=int)\n",
        "        for i in range(0, len(self.PRED_LABEL)):\n",
        "            if(self.df[self.PRED_LABEL[i].strip()].iloc[idx].astype('int') > 0):\n",
        "                label[i] = self.df[self.PRED_LABEL[i].strip()\n",
        "                                  ].iloc[idx].astype('int')\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image"
      ],
      "metadata": {
        "id": "Aj7pYrvIGe4R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision\n",
        "from torch import nn\n",
        "\n",
        "from lightly.loss import NegativeCosineSimilarity\n",
        "from lightly.models.modules import SimSiamPredictionHead, SimSiamProjectionHead\n",
        "from lightly.transforms import FastSiamTransform"
      ],
      "metadata": {
        "id": "0NNZaW4XGhho"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Optional, Tuple, Union\n",
        "\n",
        "import torchvision.transforms as T\n",
        "from PIL.Image import Image\n",
        "from torch import Tensor\n",
        "\n",
        "from lightly.transforms.gaussian_blur import GaussianBlur\n",
        "from lightly.transforms.multi_view_transform import MultiViewTransform\n",
        "from lightly.transforms.rotation import random_rotation_transform\n",
        "from lightly.transforms.utils import IMAGENET_NORMALIZE\n",
        "\n",
        "\n",
        "class SimSiamTransform(MultiViewTransform):\n",
        "    \"\"\"Implements the transformations for SimSiam.\n",
        "\n",
        "    Input to this transform:\n",
        "        PIL Image or Tensor.\n",
        "\n",
        "    Output of this transform:\n",
        "        List of Tensor of length 2.\n",
        "\n",
        "    Applies the following augmentations by default:\n",
        "        - Random resized crop\n",
        "        - Random horizontal flip\n",
        "        - Color jitter\n",
        "        - Random gray scale\n",
        "        - Gaussian blur\n",
        "        - ImageNet normalization\n",
        "\n",
        "    Attributes:\n",
        "        input_size:\n",
        "            Size of the input image in pixels.\n",
        "        cj_prob:\n",
        "            Probability that color jitter is applied.\n",
        "        cj_strength:\n",
        "            Strength of the color jitter. `cj_bright`, `cj_contrast`, `cj_sat`, and\n",
        "            `cj_hue` are multiplied by this value. For datasets with small images,\n",
        "            such as CIFAR, it is recommended to set `cj_strength` to 0.5.\n",
        "        cj_bright:\n",
        "            How much to jitter brightness.\n",
        "        cj_contrast:\n",
        "            How much to jitter constrast.\n",
        "        cj_sat:\n",
        "            How much to jitter saturation.\n",
        "        cj_hue:\n",
        "            How much to jitter hue.\n",
        "        min_scale:\n",
        "            Minimum size of the randomized crop relative to the input_size.\n",
        "        random_gray_scale:\n",
        "            Probability of conversion to grayscale.\n",
        "        gaussian_blur:\n",
        "            Probability of Gaussian blur.\n",
        "        kernel_size:\n",
        "            Will be deprecated in favor of `sigmas` argument. If set, the old behavior applies and `sigmas` is ignored.\n",
        "            Used to calculate sigma of gaussian blur with kernel_size * input_size.\n",
        "        sigmas:\n",
        "            Tuple of min and max value from which the std of the gaussian kernel is sampled.\n",
        "            Is ignored if `kernel_size` is set.\n",
        "        vf_prob:\n",
        "            Probability that vertical flip is applied.\n",
        "        hf_prob:\n",
        "            Probability that horizontal flip is applied.\n",
        "        rr_prob:\n",
        "            Probability that random rotation is applied.\n",
        "        rr_degrees:\n",
        "            Range of degrees to select from for random rotation. If rr_degrees is None,\n",
        "            images are rotated by 90 degrees. If rr_degrees is a (min, max) tuple,\n",
        "            images are rotated by a random angle in [min, max]. If rr_degrees is a\n",
        "            single number, images are rotated by a random angle in\n",
        "            [-rr_degrees, +rr_degrees]. All rotations are counter-clockwise.\n",
        "        normalize:\n",
        "            Dictionary with 'mean' and 'std' for torchvision.transforms.Normalize.\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        input_size: int = 224,\n",
        "        cj_prob: float = 0.8,\n",
        "        cj_strength: float = 1.0,\n",
        "        cj_bright: float = 0.4,\n",
        "        cj_contrast: float = 0.4,\n",
        "        cj_sat: float = 0.4,\n",
        "        cj_hue: float = 0.1,\n",
        "        min_scale: float = 0.2,\n",
        "        random_gray_scale: float = 0.2,\n",
        "#         gaussian_blur: float = 0.5,\n",
        "        kernel_size: Optional[float] = None,\n",
        "        sigmas: Tuple[float, float] = (0.1, 2),\n",
        "        vf_prob: float = 0.0,\n",
        "        hf_prob: float = 1.0,\n",
        "        rr_prob: float = 1.0,\n",
        "        rr_degrees: Union[None, float, Tuple[float, float]] = 10,\n",
        "        normalize: Union[None, dict] = IMAGENET_NORMALIZE,\n",
        "    ):\n",
        "        view_transform = SimSiamViewTransform(\n",
        "            input_size=input_size,\n",
        "            cj_prob=cj_prob,\n",
        "            cj_strength=cj_strength,\n",
        "            cj_bright=cj_bright,\n",
        "            cj_contrast=cj_contrast,\n",
        "            cj_sat=cj_sat,\n",
        "            cj_hue=cj_hue,\n",
        "            min_scale=min_scale,\n",
        "            random_gray_scale=random_gray_scale,\n",
        "#             gaussian_blur=gaussian_blur,\n",
        "            kernel_size=kernel_size,\n",
        "            sigmas=sigmas,\n",
        "            vf_prob=vf_prob,\n",
        "            hf_prob=hf_prob,\n",
        "            rr_prob=rr_prob,\n",
        "            rr_degrees=rr_degrees,\n",
        "            normalize=normalize,\n",
        "        )\n",
        "        super().__init__(transforms=[view_transform, view_transform])\n",
        "\n",
        "\n",
        "class SimSiamViewTransform:\n",
        "    def __init__(\n",
        "        self,\n",
        "        input_size: int = 224,\n",
        "        cj_prob: float = 0.8,\n",
        "        cj_strength: float = 1.0,\n",
        "        cj_bright: float = 0.4,\n",
        "        cj_contrast: float = 0.4,\n",
        "        cj_sat: float = 0.4,\n",
        "        cj_hue: float = 0.1,\n",
        "        min_scale: float = 0.2,\n",
        "        random_gray_scale: float = 0.2,\n",
        "#         gaussian_blur: float = 0.5,\n",
        "        kernel_size: Optional[float] = None,\n",
        "        sigmas: Tuple[float, float] = (0.1, 2),\n",
        "        vf_prob: float = 0.0,\n",
        "        hf_prob: float = 1.0,\n",
        "        rr_prob: float = 1.0,\n",
        "        rr_degrees: Union[None, float, Tuple[float, float]] = 10,\n",
        "        normalize: Union[None, dict] = IMAGENET_NORMALIZE,\n",
        "    ):\n",
        "        color_jitter = T.ColorJitter(\n",
        "            brightness=cj_strength * cj_bright,\n",
        "            contrast=cj_strength * cj_contrast,\n",
        "            saturation=cj_strength * cj_sat,\n",
        "            hue=cj_strength * cj_hue,\n",
        "        )\n",
        "\n",
        "        transform = [\n",
        "            T.Resize(224),\n",
        "            # T.CenterCrop(224),\n",
        "#             T.RandomResizedCrop(size=input_size, scale=(min_scale, 0.8)),\n",
        "            random_rotation_transform(rr_prob=rr_prob, rr_degrees=rr_degrees),\n",
        "            T.RandomHorizontalFlip(p=hf_prob),\n",
        "#             T.RandomVerticalFlip(p=vf_prob),\n",
        "#             T.RandomApply([color_jitter], p=cj_prob),\n",
        "#             T.RandomGrayscale(p=random_gray_scale),\n",
        "#             GaussianBlur(kernel_size=kernel_size, sigmas=sigmas, prob=gaussian_blur),\n",
        "            T.ToTensor(),\n",
        "        ]\n",
        "        if normalize:\n",
        "            transform += [T.Normalize(mean=normalize[\"mean\"], std=normalize[\"std\"])]\n",
        "        self.transform = T.Compose(transform)\n",
        "\n",
        "    def __call__(self, image: Union[Tensor, Image]) -> Tensor:\n",
        "        \"\"\"\n",
        "        Applies the transforms to the input image.\n",
        "\n",
        "        Args:\n",
        "            image:\n",
        "                The input image to apply the transforms to.\n",
        "\n",
        "        Returns:\n",
        "            The transformed image.\n",
        "\n",
        "        \"\"\"\n",
        "        return self.transform(image)"
      ],
      "metadata": {
        "id": "a1uJSWhpGjOp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transform = SimSiamTransform()\n",
        "\n",
        "dataset = CreateDataset(train, 'train', transform=transform)\n",
        "\n",
        "dataloader = torch.utils.data.DataLoader(\n",
        "    dataset,\n",
        "    batch_size=64,\n",
        "    shuffle=True,\n",
        "    drop_last=True,\n",
        "    num_workers=8,\n",
        "    pin_memory=True\n",
        ")"
      ],
      "metadata": {
        "id": "V4MudHKGHRD0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision\n",
        "from torch import nn\n",
        "\n",
        "from lightly.loss import NegativeCosineSimilarity\n",
        "from lightly.models.modules import SimSiamPredictionHead, SimSiamProjectionHead\n",
        "# from lightly.transforms import SimSiamTransform\n",
        "\n",
        "\n",
        "class SimSiam_Train(nn.Module):\n",
        "    def __init__(self, backbone):\n",
        "        super().__init__()\n",
        "        self.backbone = backbone\n",
        "        self.projection_head = SimSiamProjectionHead(2048, 512, 128)\n",
        "        self.prediction_head = SimSiamPredictionHead(128, 64, 128)\n",
        "\n",
        "    def forward(self, x):\n",
        "        f = self.backbone(x).flatten(start_dim=1)\n",
        "        z = self.projection_head(f)\n",
        "        p = self.prediction_head(z)\n",
        "        z = z.detach()\n",
        "        return z, p"
      ],
      "metadata": {
        "id": "S2FoVCbCHzd5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Copyright (c) Facebook, Inc. and its affiliates.\n",
        "# All rights reserved.\n",
        "\n",
        "# This source code is licensed under the license found in the\n",
        "# LICENSE file in the root directory of this source tree.\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "\n",
        "class SimSiam(nn.Module):\n",
        "    \"\"\"\n",
        "    Build a SimSiam model.\n",
        "    \"\"\"\n",
        "    def __init__(self, base_encoder, dim=2048, pred_dim=512):\n",
        "        \"\"\"\n",
        "        dim: feature dimension (default: 2048)\n",
        "        pred_dim: hidden dimension of the predictor (default: 512)\n",
        "        \"\"\"\n",
        "        super(SimSiam, self).__init__()\n",
        "\n",
        "        # create the encoder\n",
        "        # num_classes is the output fc dimension, zero-initialize last BNs\n",
        "        self.encoder = base_encoder(num_classes=dim, zero_init_residual=True)\n",
        "\n",
        "        # build a 3-layer projector\n",
        "        prev_dim = self.encoder.fc.weight.shape[1]\n",
        "        self.encoder.fc = nn.Sequential(nn.Linear(prev_dim, prev_dim, bias=False),\n",
        "                                        nn.BatchNorm1d(prev_dim),\n",
        "                                        nn.ReLU(inplace=True), # first layer\n",
        "                                        nn.Linear(prev_dim, prev_dim, bias=False),\n",
        "                                        nn.BatchNorm1d(prev_dim),\n",
        "                                        nn.ReLU(inplace=True), # second layer\n",
        "                                        self.encoder.fc,\n",
        "                                        nn.BatchNorm1d(dim, affine=False)) # output layer\n",
        "        self.encoder.fc[6].bias.requires_grad = False # hack: not use bias as it is followed by BN\n",
        "\n",
        "        # build a 2-layer predictor\n",
        "        self.predictor = nn.Sequential(nn.Linear(dim, pred_dim, bias=False),\n",
        "                                        nn.BatchNorm1d(pred_dim),\n",
        "                                        nn.ReLU(inplace=True), # hidden layer\n",
        "                                        nn.Linear(pred_dim, dim)) # output layer\n",
        "\n",
        "    def forward(self, x1, x2):\n",
        "        \"\"\"\n",
        "        Input:\n",
        "            x1: first views of images\n",
        "            x2: second views of images\n",
        "        Output:\n",
        "            p1, p2, z1, z2: predictors and targets of the network\n",
        "            See Sec. 3 of https://arxiv.org/abs/2011.10566 for detailed notations\n",
        "        \"\"\"\n",
        "\n",
        "        # compute features for one view\n",
        "        z1 = self.encoder(x1) # NxC\n",
        "        z2 = self.encoder(x2) # NxC\n",
        "\n",
        "        p1 = self.predictor(z1) # NxC\n",
        "        p2 = self.predictor(z2) # NxC\n",
        "\n",
        "        return p1, p2, z1.detach(), z2.detach()"
      ],
      "metadata": {
        "id": "Kk4dAnMVOzJ2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = SimSiam(\n",
        "    models.__dict__['resnet50'],\n",
        "    2048, 512)\n",
        "\n",
        "model = nn.DataParallel(model)\n",
        "\n",
        "# ckpt = torch.load('/kaggle/input/chestx-ray/fastsiam_2_resnet50_backbonemodel.pth')\n",
        "# backbone.load_state_dict(ckpt['resnet50_parameters'])\n",
        "\n",
        "checkpoint = torch.load('/content/drive/MyDrive/simsiam/checkpoint_0099.pth.tar')\n",
        "model.load_state_dict(checkpoint['state_dict'])\n",
        "\n",
        "backbone = nn.Sequential(*list(model.module.children())[:-1])\n",
        "backbone = nn.Sequential(*list(backbone[0].children())[:-1])\n",
        "\n",
        "model = SimSiam_Train(backbone)\n",
        "\n",
        "# checkpoint = torch.load('/content/pretrained_simsiam_28_withprojectionhead.pth')\n",
        "# model.load_state_dict(checkpoint)\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model = model.to(device)\n",
        "# model.load_state_dict(torch.load('/kaggle/input/chestx-ray/fastsiam_16_resnet50_withprojectionhead.pth'))\n",
        "# model.to(device)\n",
        "print('Model ready for training')"
      ],
      "metadata": {
        "id": "ivovtRm_PMJP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model"
      ],
      "metadata": {
        "id": "JDdLyCX5XBZ0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# for batch in dataloader:\n",
        "#     x0, x1 = batch\n",
        "#     break"
      ],
      "metadata": {
        "id": "GcBHABZMTq4Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# images = next(iter(dataloader))"
      ],
      "metadata": {
        "id": "4lp_Nb8wTzrU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# plt.figure()\n",
        "\n",
        "# f, axarr = plt.subplots(2,1)\n",
        "\n",
        "# axarr[0].imshow(images[1][16][0])\n",
        "# axarr[1].imshow(images[0][16][1])"
      ],
      "metadata": {
        "id": "CgeCv4K7UAW8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = NegativeCosineSimilarity()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.06, momentum=0.9, weight_decay=0.0001)\n",
        "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, 40)\n",
        "loss_val_list = []\n",
        "\n",
        "print(\"Starting Training\")\n",
        "for epoch in range(40):\n",
        "    t0 = time.time()\n",
        "    total_loss = 0\n",
        "    for batch in dataloader:\n",
        "        x0, x1 = batch\n",
        "        x0 = x0.to(device)\n",
        "        x1 = x1.to(device)\n",
        "        z0, p0 = model(x0)\n",
        "        z1, p1 = model(x1)\n",
        "        loss = 0.5 * (criterion(z0, p1) + criterion(z1, p0))\n",
        "        total_loss += loss.detach()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "    scheduler.step()\n",
        "    avg_loss = total_loss / len(dataloader)\n",
        "\n",
        "    if ((epoch+1) == 1):\n",
        "      model_save_name = f'simsiam_chestxray_{epoch+1}_withprojectionhead.pth'\n",
        "      path = f'/content/drive/MyDrive/simsiam/{model_save_name}'\n",
        "      torch.save(model.state_dict(), path)\n",
        "\n",
        "    if ((epoch+1) % 4 == 0):\n",
        "      model_save_name = f'simsiam_chestxray_{epoch+1}_withprojectionhead.pth'\n",
        "      path = f'/content/drive/MyDrive/simsiam/{model_save_name}'\n",
        "      torch.save(model.state_dict(), path)\n",
        "\n",
        "    if ((epoch+1) % 2 == 0):\n",
        "      torch.save(model.state_dict(), f'pretrained_simsiam_{epoch+1}_withprojectionhead.pth')\n",
        "      state_dict = {'resnet50_parameters': model.backbone.state_dict()}\n",
        "      torch.save(state_dict, f'pretrained_simsiam_{epoch+1}_backbonemodel.pth')\n",
        "    print(f\"epoch: {epoch+1:>02}, loss: {avg_loss:.5f}. Time taken: {((time.time()-t0)/60):.3f} mins\")\n",
        "    loss_val_list.append(avg_loss)"
      ],
      "metadata": {
        "id": "eWP8a7vYH-a1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(len(loss_val_list)):\n",
        "  loss_val_list[i] = loss_val_list[i].item()\n",
        "np.save('/content/simsiam_loss_40.npy', loss_val_list)"
      ],
      "metadata": {
        "id": "5vR5k27jXeFh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Mso5S7QXEaNI"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}