{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport os\nfrom cv2 import imread, createCLAHE # read and equalize images\nfrom glob import glob\nimport matplotlib.pyplot as plt\n%matplotlib inline\nfrom pandas.core.common import flatten\n\nimport os\nimport torch\nimport pandas as pd\nfrom skimage import io, transform\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import transforms, utils\n\n# Ignore warnings\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nplt.ion()   # interactive mode","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from __future__ import print_function, division\n\n# pytorch imports\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.optim import lr_scheduler\nfrom torch.autograd import Variable\nimport torchvision\nfrom torchvision import datasets, models, transforms\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import transforms, utils\n\n# image imports\nfrom skimage import io, transform\nfrom PIL import Image\n\n# general imports\nimport os\nimport time\nfrom shutil import copyfile\nfrom shutil import rmtree\n\n# data science imports\nimport pandas as pd\nimport numpy as np\nimport csv","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from itertools import chain\nimport random\nnp.random.seed(25)\n\ndf = pd.read_csv('../input/data/Data_Entry_2017.csv')\ndf.drop(['OriginalImage[Width', 'Height]', 'OriginalImagePixelSpacing[x', 'y]', 'Unnamed: 11'], axis=1, inplace=True)\nnum_obs = len(df)\n# print('Number of observations:',num_obs)\n\nmy_glob = glob('../input/data/images*/images/*.png')\n# print('Number of Observations: ', len(my_glob))\n\nfull_img_paths = {os.path.basename(x): x for x in my_glob}\ndf['full_path'] = df['Image Index'].map(full_img_paths.get)\n\ntrain_val_list = pd.read_csv('../input/data/train_val_list.txt', header=None, names = ['image_list'])\ntest_list = pd.read_csv('../input/data/test_list.txt', header=None, names = ['image_list'])\n\ntrain = df[df['Image Index'].isin(train_val_list['image_list'].values)].reset_index(drop=True)\ntest = df[df['Image Index'].isin(test_list['image_list'].values)].reset_index(drop=True)\n\nlabels_discard = ['Consolidation', 'Edema', 'Emphysema', 'Fibrosis', 'Hernia', 'Pleural_Thickening']\nfor i in labels_discard:\n    train = train[~train['Finding Labels'].str.contains(i)]\n    test = test[~test['Finding Labels'].str.contains(i)]\n\n# train = pd.concat([train[~train['Finding Labels'].str.contains('No Finding')],\n#                   train[train['Finding Labels'].str.contains('No Finding')].drop_duplicates(subset=['Finding Labels', 'Patient ID', 'View Position'], keep='first')]).sort_values(by='Image Index')\n\ndef one_hot_enc(df):\n    df['Finding Labels'] = df['Finding Labels'].map(lambda x: x.replace('No Finding', ''))\n    all_labels = np.unique(list(chain(*df['Finding Labels'].map(lambda x: x.split('|')).tolist())))\n    for c_label in all_labels:\n        if len(c_label)>1: # leave out empty labels\n            df[c_label] = df['Finding Labels'].map(lambda finding: 1 if c_label in finding else 0)\n    return df\n        \ntrain = one_hot_enc(train)\ntest = one_hot_enc(test)\n\ntrain_image_paths = list(flatten(train['full_path'].values))\ntest_image_paths = list(flatten(test['full_path'].values))\n\nl = np.unique(train['Patient ID'].values)\nnp.random.shuffle(l)\ncut = int(np.round(((90/100)*len(l)), decimals=0))\ntrain_values = l[:cut]\nval_values = l[cut:]\ntrain_dict = dict.fromkeys(train_values, 'train')\ntrain_dict.update(dict.fromkeys(val_values, 'val'))\ntrain['fold'] = train['Patient ID'].map(train_dict.get)\n\ntrain = train[['full_path', 'Image Index', 'fold', 'Atelectasis', 'Cardiomegaly', 'Effusion', 'Infiltration',\n               'Mass', 'Nodule', 'Pneumonia', 'Pneumothorax']]\ntest['fold'] = 'test'\ntest = test[['full_path', 'Image Index', 'fold', 'Atelectasis', 'Cardiomegaly', 'Effusion', 'Infiltration',\n             'Mass', 'Nodule', 'Pneumonia', 'Pneumothorax']]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"healthy = train[train[['Atelectasis', 'Cardiomegaly', 'Effusion', 'Infiltration', 'Mass', 'Nodule', 'Pneumonia', 'Pneumothorax']].sum(axis=1) == 0]\nhealthy.reset_index(drop=True, inplace=True)\nhealthy.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"healthy[:500]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt\nimport cv2\nimport torch.optim as optim\nimport torch\nimport torchvision\nimport torch.nn as nn\nfrom torchvision import transforms\nimport torch.nn.functional as F\n\ntorch.manual_seed(98)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"im1 = cv2.imread(healthy['full_path'].iloc[0])\n# im1 = cv2.imread(\"black-line-md.png\",0)\nim1 = cv2.normalize(im1, None, alpha=0, beta=1, norm_type=cv2.NORM_MINMAX, dtype=cv2.CV_32F)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Assuming all images are the same size, get dimensions of first image\nw,h=Image.open(healthy['full_path'].iloc[0]).size\nN=len(healthy[:2000])\n\n# Create a numpy array of floats to store the average (assume RGB images)\narr=np.zeros((h,w,3),np.float)\n\n# Build up average pixel intensities, casting each image as an array of floats\nfor i in range(N):\n    im = Image.open(healthy['full_path'].iloc[i])\n    im = im.convert('RGB')\n    imarr=np.array(im,dtype=np.float)\n    arr=arr+imarr/N\n\n# Round values in array and cast as 8-bit integer\narr=np.array(np.round(arr),dtype=np.uint8)\n\n# Generate, save and preview final image\nout=Image.fromarray(arr,mode=\"RGB\")\n\ndim = (256, 256)\nout = cv2.resize(np.asarray(out), dim)\n\n# img = Image.fromarray(out, 'RGB')\nimg = out\nw, h = 224, 224\ncenter = img.shape\nx = center[1]/2 - w/2\ny = center[0]/2 - h/2\n\nanchor = img[int(y):int(y+h), int(x):int(x+w)]\n# anchor = cv2.resize(anchor, (224,224))\n# anchor = cv2.normalize(anchor, None, alpha=0, beta=1, norm_type=cv2.NORM_MINMAX, dtype=cv2.CV_32F)\n\nplt.figure()\nf, axarr = plt.subplots(1,2) \n\naxarr[0].imshow(out)\naxarr[1].imshow(anchor)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"anchor.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from PIL import Image\n\nclass CreateDataset(Dataset):\n    \n    def __init__(self, train, fold, anchor, transform=None):\n        self.df = train\n        self.transform = transform\n        self.df = self.df[self.df['fold'] == fold]\n        \n        self.df = self.df.set_index(\"Image Index\")\n        self.PRED_LABEL = ['Atelectasis',\n                           'Cardiomegaly',\n                           'Effusion',\n                           'Infiltration',\n                           'Mass',\n                           'Nodule',\n                           'Pneumonia',\n                           'Pneumothorax']\n        self.anchor = anchor\n        RESULT_PATH = \"results/\"\n    \n    def __len__(self):\n        return len(self.df)\n    \n    def __getitem__(self, idx):\n        if torch.is_tensor(idx):\n            idx = idx.tolist()\n        \n        image = Image.open(self.df.iloc[idx, 0])\n        image = image.convert('RGB')\n        anchor = self.anchor\n        anchor = anchor.convert('RGB')\n        \n        label = np.zeros(len(self.PRED_LABEL), dtype=int)\n        for i in range(0, len(self.PRED_LABEL)):\n            if(self.df[self.PRED_LABEL[i].strip()].iloc[idx].astype('int') > 0):\n                label[i] = self.df[self.PRED_LABEL[i].strip()\n                                  ].iloc[idx].astype('int')\n\n        if self.transform:\n            image = self.transform(image)\n            anchor = self.transform(anchor)\n        \n        return image, anchor","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mean = [0.485, 0.456, 0.406]\nstd = [0.229, 0.224, 0.225]\nN_LABELS = 8  # we are predicting 14 labels\ndata_transforms = {\n    'train': transforms.Compose([\n#         transforms.RandomHorizontalFlip(),\n        transforms.Resize(224),\n        # because resize doesn't always give 224 x 224, this ensures 224 x\n        # 224\n        transforms.CenterCrop(224),\n        transforms.ToTensor(),\n        transforms.Normalize(mean, std)\n    ]),\n    'val': transforms.Compose([\n        transforms.Resize(224),\n        transforms.CenterCrop(224),\n        transforms.ToTensor(),\n        transforms.Normalize(mean, std)\n    ]),\n    'test': transforms.Compose([\n        transforms.Resize(224),\n        transforms.CenterCrop(224),\n        transforms.ToTensor(),\n        transforms.Normalize(mean, std)\n    ]),\n}\n\nanchor_temp = Image.fromarray(anchor)\n# anchor_temp = anchor\n\ntransformed_datasets = {}\ntransformed_datasets['train'] = CreateDataset(healthy, 'train', anchor_temp, transform=data_transforms['test'])\ntransformed_datasets['val'] = CreateDataset(train, 'val', anchor_temp, transform=data_transforms['test'])\n\ndataloaders = {}\ndataloaders['train'] = torch.utils.data.DataLoader(\n    transformed_datasets['train'],\n    batch_size=16,\n    shuffle=True,\n    num_workers=8,\n    pin_memory=True)\ndataloaders['val'] = torch.utils.data.DataLoader(\n    transformed_datasets['val'],\n    batch_size=16,\n    shuffle=True,\n    num_workers=8,\n    pin_memory=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nfrom PIL import Image\nfrom torch.autograd import Variable\n\n\ndef load_image(filename, size=None, scale=None):\n    img = Image.open(filename)\n    if size is not None:\n        img = img.resize((size, size), Image.ANTIALIAS)\n    elif scale is not None:\n        img = img.resize((int(img.size[0] / scale), int(img.size[1] / scale)), Image.ANTIALIAS)\n    return img\n\n\ndef save_image(filename, data):\n    img = data.clone().clamp(0, 255).numpy()\n    img = img.transpose(1, 2, 0).astype(\"uint8\")\n    img = Image.fromarray(img)\n    img.save(filename)\n\n\ndef gram_matrix(y):\n    (b, ch, h, w) = y.size()\n    features = y.view(b, ch, w * h)\n    features_t = features.transpose(1, 2)\n    gram = features.bmm(features_t) / (ch * h * w)\n    return gram\n\n\ndef normalize_batch(batch):\n    # normalize using imagenet mean and std\n    mean = batch.data.new(batch.data.size())\n    std = batch.data.new(batch.data.size())\n    mean[:, 0, :, :] = 0.485\n    mean[:, 1, :, :] = 0.456\n    mean[:, 2, :, :] = 0.406\n    std[:, 0, :, :] = 0.229\n    std[:, 1, :, :] = 0.224\n    std[:, 2, :, :] = 0.225\n    batch = torch.div(batch, 255.0)\n    batch -= Variable(mean)\n    batch = batch / Variable(std)\n    return batch","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# the vgg16 is borrowed from the pytorch tutorial\n# the resent is used to reimplemented and the comparision experiments\nfrom collections import namedtuple\nimport os\nimport torch\nfrom torchvision import models\n\nclass ResNet50(torch.nn.Module):\n    def __init__(self):\n        super(ResNet50, self).__init__()\n        \n        resnet50_pretrained = models.resnet50()\n        num_ftrs = resnet50_pretrained.fc.in_features\n        resnet50_pretrained.fc = nn.Sequential(\n            nn.Linear(num_ftrs, 8), nn.Sigmoid())\n\n        resnet50_pretrained.load_state_dict(torch.load('/kaggle/input/models-for-localisation-testing/resnet50_5_chestxray8.pth'))\n        resnet50_pretrained.eval()\n        \n        self.model  = list(resnet50_pretrained.children())\n        \n        self.noslice1 = self.model[0]\n        self.noslice2 = self.model[1]\n        self.noslice3 = self.model[2]\n        self.noslice4 = self.model[3]\n        \n        self.slice1 = self.model[4]\n        self.slice2 = self.model[5]\n        self.slice3 = self.model[6]\n        self.slice4 = self.model[7]\n        \n        \n    def forward(self, x):\n#         x = x.to(device)\n        o = x\n#         for i in range(4):\n#             o = self.model[i](o)\n            \n        o = self.noslice1(o)\n        o = self.noslice2(o)\n        o = self.noslice3(o)\n        o = self.noslice4(o)\n\n        o = self.slice1(o)\n        feature1 = o\n        o = self.slice2(o)\n        feature2 = o\n        o = self.slice3(o)\n        feature3 = o\n        o = self.slice4(o)\n        feature4 = o\n#         o = self.fc(o)\n#         transform_param = o\n        \n#         return feature1, feature2, feature3, feature4\n        return feature1, feature2","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from collections import namedtuple\nimport os\nimport torch\nfrom torchvision import models\nimport torch.nn.functional as F\ntorch.manual_seed(98)\n\nclass AlignmentNetwork(torch.nn.Module):\n    def __init__(self):\n        super(AlignmentNetwork, self).__init__()\n        \n        resnet18 = models.resnet18(pretrained=True)\n        num_ftrs = resnet18.fc.in_features\n        resnet18.fc = nn.Linear(num_ftrs, 6)\n        \n        self.model  = resnet18\n        self.model.fc.weight.data.zero_()\n        self.model.fc.bias.data.copy_(torch.tensor([1,0,0,0,1,0]))\n        \n        \n    def forward(self, x):\n        theta = self.model(x)\n        theta = theta.view(-1, 2, 3).squeeze()\n        \n        self.grid = F.affine_grid(theta, x.size())\n        x = F.grid_sample(x, self.grid)\n        m = nn.AvgPool2d(16, stride=16)\n        x = m(x)\n        x = F.interpolate(x, (224,224), mode='bilinear')\n        return x","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n# model.to(device)\n\nalignment_network = AlignmentNetwork()\nalignment_network = alignment_network.to(device)\nalignment_network.train()\n\nextract_model = ResNet50()\nextract_model = extract_model.to(device)\nextract_model = extract_model.type(torch.cuda.FloatTensor)\n# extract_model.eval()\n\ncriterion = torch.nn.MSELoss(reduction='mean')\noptimizer = torch.optim.SGD(alignment_network.parameters(), lr=0.001, momentum=0.9, weight_decay=0.0001)\n# optimizer = optim.Adam(alignment_network.parameters(),lr=0.001, betas=(0.9, 0.999))\n# scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, 40)\nloss_val_list = []\n\nprint(\"Starting Training\")\nfor epoch in range(5):\n    t0 = time.time()\n    total_loss = 0\n    for batch in dataloaders['train']:\n        perceptual_loss = 0\n        euclidean_loss = 0\n        x0, anchor = batch\n        x0, anchor = x0.to(device), anchor.to(device)\n        transformed_z0 = alignment_network(x0)\n#         transformed_z0 = transform_input(z0, x0)\n#         transformed_z0 = transformed_z0.to(device)\n\n        features_anchor = extract_model(anchor)\n        features_transformed = extract_model(transformed_z0)\n                                                                                                                                                                                                     \n        optimizer.zero_grad()\n        for i in range(len(features_anchor)):\n            temp_loss = criterion(features_transformed[i], features_anchor[i])\n            perceptual_loss += temp_loss\n        \n        euclidean_loss = criterion(transformed_z0, anchor)\n        \n        loss = euclidean_loss + perceptual_loss\n\n        total_loss += loss\n        loss.backward()\n        optimizer.step()\n    \n    avg_loss = total_loss / len(dataloaders['train'])\n    torch.save(alignment_network.state_dict(), f'2feat_no_crop_fixed_pt_alignment_network_{epoch+1}.pth')\n    print(f\"epoch: {epoch+1:>02}, loss: {avg_loss:.5f}. Time taken: {((time.time()-t0)/60):.3f} mins\")\n    loss_val_list.append(avg_loss)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"features_transformed[0].shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.imshow(features_transformed[0][0][0].cpu().detach().numpy())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from collections import namedtuple\nimport os\nimport torch\nfrom torchvision import models\nimport torch.nn.functional as F\ntorch.manual_seed(98)\n\nclass AlignmentNetwork_Val(torch.nn.Module):\n    def __init__(self):\n        super(AlignmentNetwork_Val, self).__init__()\n        \n        resnet18 = models.resnet18()\n        num_ftrs = resnet18.fc.in_features\n        resnet18.fc = nn.Linear(num_ftrs, 6)\n        \n        self.model  = resnet18\n        \n        \n    def forward(self, x):\n        theta = self.model(x)\n        theta = theta.view(-1, 2, 3).squeeze()\n        \n        self.grid = F.affine_grid(theta, x.size())\n        x = F.grid_sample(x, self.grid)\n#         m = nn.AvgPool2d(16, stride=16)\n#         x = m(x)\n#         x = F.interpolate(x, (224,224), mode='bilinear')\n        return x","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n# model.to(device)\n\nalignment_network = AlignmentNetwork_Val()\nalignment_network.load_state_dict(torch.load('/kaggle/working/2feat_no_crop_fixed_pt_alignment_network_5.pth'))\nalignment_network = alignment_network.to(device)\nalignment_network.eval()\n\nextract_model = ResNet50()\nextract_model = extract_model.to(device)\nextract_model = extract_model.type(torch.cuda.FloatTensor)\n# extract_model.eval()\n\ncriterion = torch.nn.MSELoss(reduction='mean')\noptimizer = torch.optim.SGD(alignment_network.parameters(), lr=0.001, momentum=0.9, weight_decay=0.0001)\n# optimizer = optim.Adam(alignment_network.parameters(),lr=0.001, betas=(0.9, 0.999))\n# scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, 40)\nloss_val_list = []\n\nprint(\"Starting Training\")\nfor epoch in range(5):\n    t0 = time.time()\n    total_loss = 0\n    for batch in dataloaders['val']:\n        perceptual_loss = 0\n        euclidean_loss = 0\n        x0, anchor = batch\n        x0, anchor = x0.to(device), anchor.to(device)\n        transformed_z0 = alignment_network(x0)\n#         transformed_z0 = transform_input(z0, x0)\n#         transformed_z0 = transformed_z0.to(device)\n\n#         features_anchor = extract_model(anchor)\n#         features_transformed = extract_model(transformed_z0)\n                                                                                                                                                                                                     \n#         optimizer.zero_grad()\n#         for i in range(len(features_anchor)):\n#             temp_loss = criterion(features_transformed[i], features_anchor[i])\n#             perceptual_loss += temp_loss\n        \n#         euclidean_loss = criterion(transformed_z0, anchor)\n        \n#         loss = euclidean_loss + perceptual_loss\n\n#         total_loss += loss\n#         loss.backward()\n#         optimizer.step()\n        \n        break\n    break\n    \n#     avg_loss = total_loss / len(dataloaders['train'])\n#     torch.save(alignment_network.state_dict(), f'fixed_pt_alignment_network_{epoch+1}.pth')\n#     print(f\"epoch: {epoch+1:>02}, loss: {avg_loss:.5f}. Time taken: {((time.time()-t0)/60):.3f} mins\")\n#     loss_val_list.append(avg_loss)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.imshow(np.moveaxis(x0[15].cpu().detach().numpy(), 0, 2))\nplt.axis('off')\nplt.show()\nplt.savefig('1.png')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.imshow(np.moveaxis(transformed_z0[15].cpu().detach().numpy(), 0, 2))\nplt.axis('off')\nplt.show()\nplt.savefig('2.png')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"transformed_z0.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x0.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.imshow(np.moveaxis(x0[0].cpu().detach().numpy(), 0, 2))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}