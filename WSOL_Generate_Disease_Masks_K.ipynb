{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install torchcam","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Install the most up-to-date version from GitHub\n!pip install -e git+https://github.com/frgfm/torch-cam.git#egg=torchcam","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%matplotlib inline\n# All imports\nimport matplotlib.pyplot as plt\nimport torch\nfrom torch.nn.functional import softmax, interpolate\nfrom torchvision.io.image import read_image\nfrom torchvision.models import resnet18, densenet121, resnet50\nfrom torchvision.transforms.functional import normalize, resize, to_pil_image\n\nfrom torchcam.methods import SmoothGradCAMpp, LayerCAM, GradCAMpp, GradCAM, ScoreCAM\nfrom torchcam.utils import overlay_mask","metadata":{"execution":{"iopub.status.busy":"2023-09-13T14:34:16.352858Z","iopub.execute_input":"2023-09-13T14:34:16.353245Z","iopub.status.idle":"2023-09-13T14:34:18.164582Z","shell.execute_reply.started":"2023-09-13T14:34:16.353214Z","shell.execute_reply":"2023-09-13T14:34:18.162975Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZNK3c107SymBool10guard_boolEPKcl'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n  warn(\n","output_type":"stream"}]},{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport os\nfrom cv2 import imread, createCLAHE # read and equalize images\nfrom glob import glob\nimport matplotlib.pyplot as plt\n%matplotlib inline\nfrom pandas.core.common import flatten\n\nimport os\nimport torch\nimport pandas as pd\nfrom skimage import io, transform\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import transforms, utils\n\n# Ignore warnings\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nplt.ion()   # interactive mode","metadata":{"execution":{"iopub.status.busy":"2023-09-13T14:34:18.172005Z","iopub.execute_input":"2023-09-13T14:34:18.172383Z","iopub.status.idle":"2023-09-13T14:34:20.127747Z","shell.execute_reply.started":"2023-09-13T14:34:18.172357Z","shell.execute_reply":"2023-09-13T14:34:20.126023Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n","output_type":"stream"},{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"<contextlib.ExitStack at 0x79149a0afac0>"},"metadata":{}}]},{"cell_type":"code","source":"from __future__ import print_function, division\n\n# pytorch imports\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.optim import lr_scheduler\nfrom torch.autograd import Variable\nimport torchvision\nfrom torchvision import datasets, models, transforms\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import transforms, utils\n\n# image imports\nfrom skimage import io, transform\nfrom PIL import Image\n\n# general imports\nimport os\nimport time\nfrom shutil import copyfile\nfrom shutil import rmtree\n\n# data science imports\nimport pandas as pd\nimport numpy as np\nimport csv","metadata":{"execution":{"iopub.status.busy":"2023-09-13T14:34:20.129699Z","iopub.execute_input":"2023-09-13T14:34:20.130175Z","iopub.status.idle":"2023-09-13T14:34:20.137765Z","shell.execute_reply.started":"2023-09-13T14:34:20.130124Z","shell.execute_reply":"2023-09-13T14:34:20.136275Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from itertools import chain\nimport random\nnp.random.seed(25)\n\ndf = pd.read_csv('../input/data/Data_Entry_2017.csv')\ndf.drop(['OriginalImage[Width', 'Height]', 'OriginalImagePixelSpacing[x', 'y]', 'Unnamed: 11'], axis=1, inplace=True)\nnum_obs = len(df)\n# print('Number of observations:',num_obs)\n\nmy_glob = glob('../input/data/images*/images/*.png')\n# print('Number of Observations: ', len(my_glob))\n\nfull_img_paths = {os.path.basename(x): x for x in my_glob}\ndf['full_path'] = df['Image Index'].map(full_img_paths.get)\n\ntrain_val_list = pd.read_csv('../input/data/train_val_list.txt', header=None, names = ['image_list'])\ntest_list = pd.read_csv('../input/data/test_list.txt', header=None, names = ['image_list'])\n\ntrain = df[df['Image Index'].isin(train_val_list['image_list'].values)].reset_index(drop=True)\ntest = df[df['Image Index'].isin(test_list['image_list'].values)].reset_index(drop=True)\n\nlabels_discard = ['Consolidation', 'Edema', 'Emphysema', 'Fibrosis', 'Hernia', 'Pleural_Thickening']\nfor i in labels_discard:\n    train = train[~train['Finding Labels'].str.contains(i)]\n    test = test[~test['Finding Labels'].str.contains(i)]\n\n# train = pd.concat([train[~train['Finding Labels'].str.contains('No Finding')],\n#                   train[train['Finding Labels'].str.contains('No Finding')].drop_duplicates(subset=['Finding Labels', 'Patient ID', 'View Position'], keep='first')]).sort_values(by='Image Index')\n\ndef one_hot_enc(df):\n    df['Finding Labels'] = df['Finding Labels'].map(lambda x: x.replace('No Finding', ''))\n    all_labels = np.unique(list(chain(*df['Finding Labels'].map(lambda x: x.split('|')).tolist())))\n    for c_label in all_labels:\n        if len(c_label)>1: # leave out empty labels\n            df[c_label] = df['Finding Labels'].map(lambda finding: 1 if c_label in finding else 0)\n    return df\n        \ntrain = one_hot_enc(train)\ntest = one_hot_enc(test)\n\ntrain_image_paths = list(flatten(train['full_path'].values))\ntest_image_paths = list(flatten(test['full_path'].values))\n\nl = np.unique(train['Patient ID'].values)\nnp.random.shuffle(l)\ncut = int(np.round(((90/100)*len(l)), decimals=0))\ntrain_values = l[:cut]\nval_values = l[cut:]\ntrain_dict = dict.fromkeys(train_values, 'train')\ntrain_dict.update(dict.fromkeys(val_values, 'val'))\ntrain['fold'] = train['Patient ID'].map(train_dict.get)\n\ntrain = train[['full_path', 'Image Index', 'fold', 'Atelectasis', 'Cardiomegaly', 'Effusion', 'Infiltration',\n               'Mass', 'Nodule', 'Pneumonia', 'Pneumothorax']]\ntest['fold'] = 'test'\ntest = test[['full_path', 'Image Index', 'fold', 'Atelectasis', 'Cardiomegaly', 'Effusion', 'Infiltration',\n             'Mass', 'Nodule', 'Pneumonia', 'Pneumothorax']]","metadata":{"execution":{"iopub.status.busy":"2023-09-13T14:34:20.140797Z","iopub.execute_input":"2023-09-13T14:34:20.141237Z","iopub.status.idle":"2023-09-13T14:34:22.163510Z","shell.execute_reply.started":"2023-09-13T14:34:20.141183Z","shell.execute_reply":"2023-09-13T14:34:22.161578Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"df = train[train[['Atelectasis', 'Cardiomegaly', 'Effusion', 'Infiltration', 'Mass', 'Nodule', 'Pneumonia', 'Pneumothorax']].sum(axis=1) == 1]\ndf.reset_index(drop=True, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2023-09-13T14:34:22.166093Z","iopub.execute_input":"2023-09-13T14:34:22.167268Z","iopub.status.idle":"2023-09-13T14:34:22.183287Z","shell.execute_reply.started":"2023-09-13T14:34:22.167224Z","shell.execute_reply":"2023-09-13T14:34:22.180740Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"df","metadata":{"execution":{"iopub.status.busy":"2023-09-13T14:34:22.185588Z","iopub.execute_input":"2023-09-13T14:34:22.186026Z","iopub.status.idle":"2023-09-13T14:34:22.216874Z","shell.execute_reply.started":"2023-09-13T14:34:22.185992Z","shell.execute_reply":"2023-09-13T14:34:22.215849Z"},"trusted":true},"execution_count":6,"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"                                              full_path       Image Index  \\\n0      ../input/data/images_001/images/00000001_000.png  00000001_000.png   \n1      ../input/data/images_001/images/00000005_006.png  00000005_006.png   \n2      ../input/data/images_001/images/00000008_000.png  00000008_000.png   \n3      ../input/data/images_001/images/00000008_002.png  00000008_002.png   \n4      ../input/data/images_001/images/00000010_000.png  00000010_000.png   \n...                                                 ...               ...   \n19720  ../input/data/images_012/images/00030764_000.png  00030764_000.png   \n19721  ../input/data/images_012/images/00030770_001.png  00030770_001.png   \n19722  ../input/data/images_012/images/00030780_000.png  00030780_000.png   \n19723  ../input/data/images_012/images/00030786_000.png  00030786_000.png   \n19724  ../input/data/images_012/images/00030789_000.png  00030789_000.png   \n\n        fold  Atelectasis  Cardiomegaly  Effusion  Infiltration  Mass  Nodule  \\\n0      train            0             1         0             0     0       0   \n1      train            0             0         0             1     0       0   \n2      train            0             1         0             0     0       0   \n3      train            0             0         0             0     0       1   \n4      train            0             0         0             1     0       0   \n...      ...          ...           ...       ...           ...   ...     ...   \n19720  train            0             1         0             0     0       0   \n19721  train            0             0         0             1     0       0   \n19722  train            1             0         0             0     0       0   \n19723  train            0             0         1             0     0       0   \n19724  train            0             0         0             1     0       0   \n\n       Pneumonia  Pneumothorax  \n0              0             0  \n1              0             0  \n2              0             0  \n3              0             0  \n4              0             0  \n...          ...           ...  \n19720          0             0  \n19721          0             0  \n19722          0             0  \n19723          0             0  \n19724          0             0  \n\n[19725 rows x 11 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>full_path</th>\n      <th>Image Index</th>\n      <th>fold</th>\n      <th>Atelectasis</th>\n      <th>Cardiomegaly</th>\n      <th>Effusion</th>\n      <th>Infiltration</th>\n      <th>Mass</th>\n      <th>Nodule</th>\n      <th>Pneumonia</th>\n      <th>Pneumothorax</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>../input/data/images_001/images/00000001_000.png</td>\n      <td>00000001_000.png</td>\n      <td>train</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>../input/data/images_001/images/00000005_006.png</td>\n      <td>00000005_006.png</td>\n      <td>train</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>../input/data/images_001/images/00000008_000.png</td>\n      <td>00000008_000.png</td>\n      <td>train</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>../input/data/images_001/images/00000008_002.png</td>\n      <td>00000008_002.png</td>\n      <td>train</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>../input/data/images_001/images/00000010_000.png</td>\n      <td>00000010_000.png</td>\n      <td>train</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>19720</th>\n      <td>../input/data/images_012/images/00030764_000.png</td>\n      <td>00030764_000.png</td>\n      <td>train</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>19721</th>\n      <td>../input/data/images_012/images/00030770_001.png</td>\n      <td>00030770_001.png</td>\n      <td>train</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>19722</th>\n      <td>../input/data/images_012/images/00030780_000.png</td>\n      <td>00030780_000.png</td>\n      <td>train</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>19723</th>\n      <td>../input/data/images_012/images/00030786_000.png</td>\n      <td>00030786_000.png</td>\n      <td>train</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>19724</th>\n      <td>../input/data/images_012/images/00030789_000.png</td>\n      <td>00030789_000.png</td>\n      <td>train</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>19725 rows × 11 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"from collections import namedtuple\nimport os\nimport torch\nfrom torchvision import models\nimport torch.nn.functional as F\ntorch.manual_seed(98)\n\nclass AlignmentNetwork(torch.nn.Module):\n    def __init__(self):\n        super(AlignmentNetwork, self).__init__()\n\n        resnet18 = models.resnet18()\n        num_ftrs = resnet18.fc.in_features\n        resnet18.fc = nn.Linear(num_ftrs, 6)\n\n        self.model  = resnet18\n\n\n    def forward(self, x):\n        theta = self.model(x)\n        theta = theta.view(-1, 2, 3)\n\n        self.grid = F.affine_grid(theta, x.size())\n        x = F.grid_sample(x, self.grid)\n#         m = nn.AvgPool2d(16, stride=16)\n#         x = m(x)\n#         x = F.interpolate(x, (224,224), mode='bilinear')\n        return x","metadata":{"execution":{"iopub.status.busy":"2023-09-13T14:43:17.682663Z","iopub.execute_input":"2023-09-13T14:43:17.683097Z","iopub.status.idle":"2023-09-13T14:43:17.692363Z","shell.execute_reply.started":"2023-09-13T14:43:17.683064Z","shell.execute_reply":"2023-09-13T14:43:17.691248Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"# the vgg16 is borrowed from the pytorch tutorial\n# the resent is used to reimplemented and the comparision experiments\nfrom collections import namedtuple\nimport os\nimport torch\nfrom torchvision import models\n\nclass ResNet50(torch.nn.Module):\n    def __init__(self):\n        super(ResNet50, self).__init__()\n\n        resnet50_pretrained = models.resnet50()\n        num_ftrs = resnet50_pretrained.fc.in_features\n        resnet50_pretrained.fc = nn.Sequential(\n            nn.Linear(num_ftrs, 8), nn.Sigmoid())\n\n        # resnet50_pretrained.load_state_dict(torch.load('/kaggle/input/models-for-localisation-testing/resnet50_5_chestxray8.pth'))\n\n        self.model  = list(resnet50_pretrained.children())\n\n        self.noslice1 = self.model[0]\n        self.noslice2 = self.model[1]\n        self.noslice3 = self.model[2]\n        self.noslice4 = self.model[3]\n\n        self.slice1 = self.model[4]\n        self.slice2 = self.model[5]\n        self.slice3 = self.model[6]\n        self.slice4 = self.model[7]\n\n#         self.fc = nn.Sequential(nn.AdaptiveAvgPool2d((1,1)),\n        self.conv = nn.Sequential(nn.Conv2d(in_channels=2048, out_channels=8, kernel_size=1))\n                                # nn.ReLU(inplace=True))\n        self.anothaone = nn.Sequential(nn.Conv2d(in_channels=8, out_channels=8, kernel_size=1))\n                                # nn.ReLU(inplace=True))\n        self.fc = nn.Sequential(nn.AdaptiveAvgPool2d((1,1)), nn.Sigmoid())\n        # self.fc = nn.Sequential(nn.AdaptiveAvgPool2d((1,1)),\n        #                         nn.Flatten(),\n        #                         nn.Linear(2048, 8),\n        #                         nn.Sigmoid())\n\n    def forward(self, x, alignment_network):\n#         x = x.to(device)\n        o = x\n#         for i in range(4):\n#             o = self.model[i](o)\n\n        alignment_network.eval()\n        o = alignment_network(o)\n        aligned = o\n\n        o = self.noslice1(o)\n        o = self.noslice2(o)\n        o = self.noslice3(o)\n        o = self.noslice4(o)\n\n        # o = (o * lung_mask) + o\n\n#         anchor = cv.resize()\n#         o = o*anchor\n        o = self.slice1(o)\n        feature1 = o\n        # o = (o * lung_mask) + o\n        o = self.slice2(o)\n        feature2 = o\n        o = self.slice3(o)\n        feature3 = o\n        o = self.slice4(o)\n        feature4 = o\n        o = self.conv(o)\n        final = o\n        if(o.shape[0] == 32):\n            o = (o * batch_masks) + o\n        else:\n            temp = o.shape[0]\n            o = (o * batch_masks[:temp]) + o\n        o = self.anothaone(o)\n        final2 = o\n        o = self.fc(o)\n        final3 = o\n\n        return final3","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\nmodel = ResNet50()\nmodel = model.to(device)\nmodel = model.type(torch.cuda.FloatTensor)\nmodel.load_state_dict(torch.load('/content/drive/MyDrive/final_models/no_align_5.pth'))\nmodel.eval()\nprint('Model loaded')\n# model.load_state_dict(torch.load('/content/drive/MyDrive/final_models/graddmask_new_align_10.pth'))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from torchvision import models\nimport torch.nn as nn\n\nmodel = models.resnet50()\nnum_ftrs = model.fc.in_features\nmodel.fc = nn.Sequential(\n    nn.Linear(num_ftrs, 8), nn.Sigmoid())\n\nmodel.load_state_dict(torch.load('/kaggle/input/models-for-localisation-testing/resnet50_5_chestxray8.pth', map_location=torch.device('cpu')))\nmodel.eval()\nprint('Model loaded')","metadata":{"execution":{"iopub.status.busy":"2023-09-13T14:42:41.203539Z","iopub.execute_input":"2023-09-13T14:42:41.204004Z","iopub.status.idle":"2023-09-13T14:42:41.726295Z","shell.execute_reply.started":"2023-09-13T14:42:41.203968Z","shell.execute_reply":"2023-09-13T14:42:41.725181Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stdout","text":"Model loaded\n","output_type":"stream"}]},{"cell_type":"code","source":"def large_to_small(x1, y1, w1, h1, cropped = True): # convert 1024x1024 to 224x224 (which is center-cropped from 256x256) \n    x2 = x1 / 4\n    y2 = y1 / 4\n    w2 = w1 / 4\n    h2 = h1 / 4\n    if cropped:\n        if x2 < 16:\n            x2 = 0\n            w2 = w2 - 16\n        else:\n            x2 = x2 - 16\n        if x2 + w2 > 224:\n            w2 = 224 - x2\n        if y2 < 16:\n            y2 = 0\n            h2 = h2 - 16\n        else:\n            y2 = y2 - 16\n        if y2 + h2 > 224:\n            h2 = 224 - y2\n    return int(x2), int(y2), int(w2), int(h2)\n\ndef intersect(box_a, box_b):\n    A = box_a.size(0)\n    B = box_b.size(0)\n    max_xy = torch.min(box_a[:, 2:].unsqueeze(1).expand(A, B, 2),\n                       box_b[:, 2:].unsqueeze(0).expand(A, B, 2))\n    min_xy = torch.max(box_a[:, :2].unsqueeze(1).expand(A, B, 2),\n                       box_b[:, :2].unsqueeze(0).expand(A, B, 2))\n    inter = torch.clamp((max_xy - min_xy), min=0)\n    return inter[:, :, 0] * inter[:, :, 1]\n\ndef jaccard(box_a, box_b):\n    inter = intersect(box_a, box_b)\n    area_a = ((box_a[:, 2]-box_a[:, 0]) *\n              (box_a[:, 3]-box_a[:, 1])).unsqueeze(1).expand_as(inter)  # [A,B]\n    area_b = ((box_b[:, 2]-box_b[:, 0]) *\n              (box_b[:, 3]-box_b[:, 1])).unsqueeze(0).expand_as(inter)  # [A,B]\n    union = area_a + area_b - inter\n    return inter / union  # [A,B]\n\ndef iobb(box_a, box_b):\n    inter = intersect(box_a, box_b)\n    area_a = ((box_a[:, 2]-box_a[:, 0]) *\n              (box_a[:, 3]-box_a[:, 1])).unsqueeze(1).expand_as(inter)  # [A,B]\n    area_b = ((box_b[:, 2]-box_b[:, 0]) *\n              (box_b[:, 3]-box_b[:, 1])).unsqueeze(0).expand_as(inter)  # [A,B]\n#     union = area_a + area_b - inter\n    return inter / area_a  # [A,B]\n\ndef contains(xywh1, xywh2): # returns True if xywh2 is completely inside xywh1\n    x1, x2, x3, x4 = xywh1\n    y1, y2, y3, y4 = xywh2\n    if y1 < x1:\n        return False\n    if y2 < x2:\n        return False\n    if y3 > x3:\n        return False\n    if y4 > x4:\n        return False\n    return True","metadata":{"execution":{"iopub.status.busy":"2023-09-13T14:34:30.105507Z","iopub.execute_input":"2023-09-13T14:34:30.105864Z","iopub.status.idle":"2023-09-13T14:34:30.125625Z","shell.execute_reply.started":"2023-09-13T14:34:30.105833Z","shell.execute_reply":"2023-09-13T14:34:30.122719Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"import torchvision.transforms as T\n\npreprocess = T.Compose([\n   T.Resize(224),\n   T.CenterCrop(224),\n   T.ToTensor(),\n   T.Normalize(\n       mean=[0.485, 0.456, 0.406],\n       std=[0.229, 0.224, 0.225]\n   )\n])\n\nconvert_tensor = T.Compose([\n   T.ToTensor(),\n])","metadata":{"execution":{"iopub.status.busy":"2023-09-13T14:34:31.341535Z","iopub.execute_input":"2023-09-13T14:34:31.343074Z","iopub.status.idle":"2023-09-13T14:34:31.349621Z","shell.execute_reply.started":"2023-09-13T14:34:31.343015Z","shell.execute_reply":"2023-09-13T14:34:31.348852Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"if 2==5 or 2==3:\n    print(True)","metadata":{"execution":{"iopub.status.busy":"2023-09-13T16:25:39.869567Z","iopub.execute_input":"2023-09-13T16:25:39.870933Z","iopub.status.idle":"2023-09-13T16:25:39.877311Z","shell.execute_reply.started":"2023-09-13T16:25:39.870872Z","shell.execute_reply":"2023-09-13T16:25:39.875887Z"},"trusted":true},"execution_count":42,"outputs":[]},{"cell_type":"code","source":"%%time\n\nimport io\nimport requests\nimport torchvision.transforms as T\nimport numpy as np\nfrom PIL import Image\n\nfrom torchvision import transforms\nfrom torchvision.utils import draw_segmentation_masks\nfrom torchvision.ops import masks_to_boxes\nfrom torchvision.utils import draw_bounding_boxes\nfrom matplotlib.patches import Rectangle\n\nfrom skimage.filters import threshold_otsu\n\n# cam_list = [GradCAM, GradCAMpp, LayerCAM]\ncam_list = [LayerCAM]\niou_gcam, iou_gcampp, iou_lcam = [], [], []\ncontain_gcam, contain_gcampp, contain_lcam = [], [], []\npred_class = []\nall_cams = []\nprob_value = []\ndis_0, dis_1, dis_2, dis_3, dis_4, dis_5, dis_6, dis_7 = [], [], [], [], [], [], [], []\n\nthresh = 0.8\n\nalignment_network = AlignmentNetwork()\nalignment_network.load_state_dict(torch.load('/kaggle/input/final-testing/2feat_no_crop_fixed_pt_alignment_network_5.pth', map_location=torch.device('cpu')))\n# alignment_network.load_state_dict(torch.load('/content/drive/MyDrive/fixed_pt_alignment_network_5.pth'))\n# alignment_network = alignment_network.to(device)\nalignment_network.eval()\n\nfor file in range(len(df)):\n    if file % 500 == 0:\n        print(file)\n    for f in cam_list:\n        temp_cams = []\n#         cam_name = str(f).split('.')[3].split(\"'\")[0]\n        img_path = df['full_path'].iloc[file]\n#         cam_extractor = f(model)\n        img = Image.open(img_path)\n        img = img.convert('RGB')\n        img = preprocess(img)\n        x  = alignment_network(img.unsqueeze(0))\n        out = model(x)\n        \n#         out = out[0][:8].unsqueeze(0)\n#         cams = cam_extractor(out.squeeze(0).argmax().item(), out)\n#         cam_extractor.remove_hooks()\n        for i in range(8):\n            if (i==0):\n                if(out.squeeze(0)[i] > thresh):\n                    cam_extractor = f(model)\n                    ind_out = model(img.unsqueeze(0))\n                    ind_cam = cam_extractor(i, ind_out)\n                    cam_extractor.remove_hooks()\n                    dis_0.append(ind_cam)\n                    \n            elif (i==1):\n                if(out.squeeze(0)[i] > thresh):\n                    cam_extractor = f(model)\n                    ind_out = model(img.unsqueeze(0))\n                    ind_cam = cam_extractor(i, ind_out)\n                    cam_extractor.remove_hooks()\n                    dis_1.append(ind_cam)\n                    \n            elif (i==2):\n                if(out.squeeze(0)[i] > thresh):\n                    cam_extractor = f(model)\n                    ind_out = model(img.unsqueeze(0))\n                    ind_cam = cam_extractor(i, ind_out)\n                    cam_extractor.remove_hooks()\n                    dis_2.append(ind_cam)\n                    \n            elif (i==3):\n                if(out.squeeze(0)[i] > (thresh - 0.1)):\n                    cam_extractor = f(model)\n                    ind_out = model(img.unsqueeze(0))\n                    ind_cam = cam_extractor(i, ind_out)\n                    cam_extractor.remove_hooks()\n                    dis_3.append(ind_cam)\n                    \n            elif (i==4):\n                if(out.squeeze(0)[i] > thresh):\n                    cam_extractor = f(model)\n                    ind_out = model(img.unsqueeze(0))\n                    ind_cam = cam_extractor(i, ind_out)\n                    cam_extractor.remove_hooks()\n                    dis_4.append(ind_cam)\n                    \n            elif (i==5):\n                if(out.squeeze(0)[i] > thresh):\n                    cam_extractor = f(model)\n                    ind_out = model(img.unsqueeze(0))\n                    ind_cam = cam_extractor(i, ind_out)\n                    cam_extractor.remove_hooks()\n                    dis_5.append(ind_cam)\n\n            elif (i==6):\n                if(out.squeeze(0)[i] > (thresh - 0.3)) or (out.squeeze(0).argmax().item() == 6):\n                    cam_extractor = f(model)\n                    ind_out = model(img.unsqueze(0))\n                    ind_cam = cam_extractor(i, ind_out)\n                    cam_extractor.remove_hooks()\n                    dis_6.append(ind_cam)\n                    \n            elif (i==7):\n                if(out.squeeze(0)[i] > thresh):\n                    cam_extractor = f(model)\n                    ind_out = model(img.unsqueeze(0))\n                    ind_cam = cam_extractor(i, ind_out)\n                    cam_extractor.remove_hooks()\n                    dis_7.append(ind_cam)","metadata":{"execution":{"iopub.status.busy":"2023-09-13T18:14:18.773820Z","iopub.execute_input":"2023-09-13T18:14:18.774301Z","iopub.status.idle":"2023-09-13T19:14:01.977593Z","shell.execute_reply.started":"2023-09-13T18:14:18.774264Z","shell.execute_reply":"2023-09-13T19:14:01.976268Z"},"trusted":true},"execution_count":85,"outputs":[{"name":"stdout","text":"0\n500\n1000\n1500\n2000\n2500\n3000\n3500\n4000\n4500\n5000\n5500\n6000\n6500\n7000\n7500\n8000\n8500\n9000\n9500\n10000\n10500\n11000\n11500\n12000\n12500\n13000\n13500\n14000\n14500\n15000\n15500\n16000\n16500\n17000\n17500\n18000\n18500\n19000\n19500\nCPU times: user 1h 49min 51s, sys: 26.5 s, total: 1h 50min 17s\nWall time: 59min 43s\n","output_type":"stream"}]},{"cell_type":"code","source":"g_dis_0, g_dis_1, g_dis_2, g_dis_3, g_dis_4, g_dis_5, g_dis_6, g_dis_7 = dis_0, dis_1, dis_2, dis_3, dis_4, dis_5, dis_6, dis_7","metadata":{"execution":{"iopub.status.busy":"2023-09-13T19:14:01.981920Z","iopub.execute_input":"2023-09-13T19:14:01.983436Z","iopub.status.idle":"2023-09-13T19:14:02.015679Z","shell.execute_reply.started":"2023-09-13T19:14:01.983334Z","shell.execute_reply":"2023-09-13T19:14:02.014025Z"},"trusted":true},"execution_count":86,"outputs":[]},{"cell_type":"code","source":"a0, a1, a2, a3, a4, a5, a6, a7 = [], [], [], [], [], [], [], []\nfor i in g_dis_0:\n    temp_i = np.flip(i[0][0].numpy(),1)\n    a0.append(np.add(i[0][0], temp_i))\nfor i in g_dis_1:\n    a1.append(i[0][0])\nfor i in g_dis_2:\n    temp_i = np.flip(i[0][0].numpy(),1)\n    a2.append(np.add(i[0][0], temp_i))\nfor i in g_dis_3:\n    temp_i = np.flip(i[0][0].numpy(),1)\n    a3.append(np.add(i[0][0], temp_i))\nfor i in g_dis_4:\n    temp_i = np.flip(i[0][0].numpy(),1)\n    a4.append(np.add(i[0][0], temp_i))\nfor i in g_dis_5:\n    temp_i = np.flip(i[0][0].numpy(),1)\n    a5.append(np.add(i[0][0], temp_i))\nfor i in g_dis_6:\n    temp_i = np.flip(i[0][0].numpy(),1)\n    a6.append(np.add(i[0][0], temp_i))\nfor i in g_dis_7:\n    temp_i = np.flip(i[0][0].numpy(),1)\n    a7.append(np.add(i[0][0], temp_i))","metadata":{"execution":{"iopub.status.busy":"2023-09-13T19:14:02.018902Z","iopub.execute_input":"2023-09-13T19:14:02.019314Z","iopub.status.idle":"2023-09-13T19:14:02.092698Z","shell.execute_reply.started":"2023-09-13T19:14:02.019281Z","shell.execute_reply":"2023-09-13T19:14:02.091810Z"},"trusted":true},"execution_count":87,"outputs":[]},{"cell_type":"code","source":"final_0 = np.zeros((7,7))\nfor i in a0:\n    final_0 = np.add(final_0, i)\nfinal_0 = final_0 / len(a0)\n# final_0 = torch.Tensor(final_0).unsqueeze(0)\n\nfinal_1 = np.zeros((7,7))\nfor i in a1:\n    final_1 = np.add(final_1, i)\nfinal_1 = final_1 / len(a1)\nfinal_1 = torch.Tensor(final_1).unsqueeze(0)\n\nfinal_2 = np.zeros((7,7))\nfor i in a2:\n    final_2 = np.add(final_2, i)\nfinal_2 = final_2 / len(a2)\nfinal_2 = torch.Tensor(final_2).unsqueeze(0)\n\nfinal_3 = np.zeros((7,7))\nfor i in a3:\n    final_3 = np.add(final_3, i)\nfinal_3 = final_3 / len(a3)\nfinal_3 = torch.Tensor(final_3).unsqueeze(0)\n\nfinal_4 = np.zeros((7,7))\nfor i in a4:\n    final_4 = np.add(final_4, i)\nfinal_4 = final_4 / len(a4)\nfinal_4 = torch.Tensor(final_4).unsqueeze(0)\n\nfinal_5 = np.zeros((7,7))\nfor i in a5:\n    final_5 = np.add(final_5, i)\nfinal_5 = final_5 / len(a5)\nfinal_5 = torch.Tensor(final_5).unsqueeze(0)\n\nfinal_6 = np.zeros((7,7))\nfor i in a6:\n    final_6 = np.add(final_6, i)\nfinal_6 = final_6 / len(a6)\nfinal_6 = torch.Tensor(final_6).unsqueeze(0)\n\nfinal_7 = np.zeros((7,7))\nfor i in a7:\n    final_7 = np.add(final_7, i)\nfinal_7 = final_7 / len(a7)\nfinal_7 = torch.Tensor(final_7).unsqueeze(0)","metadata":{"execution":{"iopub.status.busy":"2023-09-13T19:14:02.095682Z","iopub.execute_input":"2023-09-13T19:14:02.096078Z","iopub.status.idle":"2023-09-13T19:14:02.130111Z","shell.execute_reply.started":"2023-09-13T19:14:02.096046Z","shell.execute_reply":"2023-09-13T19:14:02.128571Z"},"trusted":true},"execution_count":88,"outputs":[]},{"cell_type":"code","source":"final_0 = final_0.numpy()\nfinal_1 = final_1.numpy()\nfinal_2 = final_2.numpy()\nfinal_3 = final_3.numpy()\nfinal_4 = final_4.numpy()\nfinal_5 = final_5.numpy()\nfinal_6 = final_6.numpy()\nfinal_7 = final_7.numpy()\n\nfinal_0 = (final_0 - np.min(final_0))/np.ptp(final_0)\nfinal_1 = (final_1 - np.min(final_1))/np.ptp(final_1)\nfinal_2 = (final_2 - np.min(final_2))/np.ptp(final_2)\nfinal_3 = (final_3 - np.min(final_3))/np.ptp(final_3)\nfinal_4 = (final_4 - np.min(final_4))/np.ptp(final_4)\nfinal_5 = (final_5 - np.min(final_5))/np.ptp(final_5)\nfinal_6 = (final_6 - np.min(final_6))/np.ptp(final_6)\nfinal_7 = (final_7 - np.min(final_7))/np.ptp(final_7)\n\nfinal_0 = torch.Tensor(final_0)\nfinal_1 = torch.Tensor(final_1)\nfinal_2 = torch.Tensor(final_2)\nfinal_3 = torch.Tensor(final_3)\nfinal_4 = torch.Tensor(final_4)\nfinal_5 = torch.Tensor(final_5)\nfinal_6 = torch.Tensor(final_6)\nfinal_7 = torch.Tensor(final_7)","metadata":{"execution":{"iopub.status.busy":"2023-09-13T19:14:02.132049Z","iopub.execute_input":"2023-09-13T19:14:02.133018Z","iopub.status.idle":"2023-09-13T19:14:02.147049Z","shell.execute_reply.started":"2023-09-13T19:14:02.132960Z","shell.execute_reply":"2023-09-13T19:14:02.145373Z"},"trusted":true},"execution_count":89,"outputs":[]},{"cell_type":"code","source":"final = [final_0, final_1, final_2, final_3, final_4, final_5, final_6, final_7]","metadata":{"execution":{"iopub.status.busy":"2023-09-13T19:14:02.148927Z","iopub.execute_input":"2023-09-13T19:14:02.149460Z","iopub.status.idle":"2023-09-13T19:14:02.166506Z","shell.execute_reply.started":"2023-09-13T19:14:02.149418Z","shell.execute_reply":"2023-09-13T19:14:02.164719Z"},"trusted":true},"execution_count":90,"outputs":[]},{"cell_type":"code","source":"final[0].shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"np.save('final_fixed_aligned_layercam_normalized_masks.npy', np.array(final, dtype=object), allow_pickle=True)","metadata":{"execution":{"iopub.status.busy":"2023-09-13T19:16:59.688361Z","iopub.execute_input":"2023-09-13T19:16:59.688786Z","iopub.status.idle":"2023-09-13T19:16:59.698443Z","shell.execute_reply.started":"2023-09-13T19:16:59.688753Z","shell.execute_reply":"2023-09-13T19:16:59.696607Z"},"trusted":true},"execution_count":96,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}